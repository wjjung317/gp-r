{
  "name": "Pivotal | R",
  "tagline": "A place for all things Pivotal & R",
  "body": "Topics covered\r\n==============\r\n* [Overview](#overview)\r\n* [PL/R on Greenplum & HAWQ](#plr)\r\n  * [Getting Started](#plr_gettingstarted)\r\n       * [PL/R Architecture](#plr_arch)\r\n       * [PL/R Installation](#installation)\r\n       * [Note on Permissions](#permissions)\r\n  * [Leveraging R Packages](#packages)\r\n       * [Checking R Package Availability](#plr_packages_check)\r\n       * [Installing R Packages](#plr_packages_install)\r\n       * [Note on R Package Versions & Dependencies](#plr_packages_versions)\r\n  * [Usage & Best Practices](#bestpractices)\r\n       * [Make a Plan](#makeplan)\r\n       * [Data Preparation](#dataprep)\r\n       * [Return types](#returntypes)\r\n       * [PL/R UDF Definition](#udf)\r\n       * [PL/R Execution](#execution)\r\n       * [Persisting R Models in the Database](#persistence)\r\n       * [Verify Parallelization](#parallelization)\r\n             * [Option 1: Via Segment Hostnames](#plr_parallelization_hostnames)\r\n             * [Option 2: Via Timing](#plr_parallelization_timing)\r\n             * [Option 3: Via Pivotal Command Center](#plr_parallelization_cc)\r\n  * [More Details](#plr_details)\r\n       * [Data Types](#datatypes)\r\n             * [PL/R Input Conversion: SQL Data Types → R Data Types](#plr_datatypes_input)\r\n             * [PL/R Output Conversion: R Data Types → SQL Data Types](#plr_datatypes_output)\r\n       * [Memory Limits](#memory)\r\n  * [Exercises](#plrexercises)\r\n       * [PL/R Exercises](#plrexercises)\r\n* [RPostgreSQL on Greenplum & HAWQ](#rpostgresql)\r\n  * [Introduction](#rpostgresql)\r\n  * [Local Development](#rpostgresql_local)\r\n  * [Plotting](#plotting)\r\n  * [Caveats Around Usage Within PL/R](#rpostgresql_plrcaveats)\r\n* [PivotalR on Greenplum & HAWQ](#pivotalr)\r\n  * [Introduction](#pivotalr)\r\n  * [Design & Features](#pivotalr_design)\r\n  * [Demo](#pivotalr_demo)\r\n  * [Download & Installation](#pivotalr_install)\r\n* [Shiny Apps on Cloud Foundry](#shiny_cf)\r\n\r\n  \r\n# <a name=\"overview\"/> Overview \r\nIn a traditional analytics workflow using R, data are loaded from a data source, modeled or visualized, and the model scoring results are pushed back to the data source. Such an approach works well when (i) the amount of data can be loaded into memory, and (ii) the transfer of large amounts of data is inexpensive and/or fast.  One of the major focus areas of this guide is to explore the situation involving large data sets where these two assumptions are violated. \r\n\r\n[Greenplum Database (GPDB)](http://pivotal.io/big-data/pivotal-greenplum) and [Apache HAWQ](http://hawq.incubator.apache.org) offer several alternatives to interact with R using the in-database/in-Hadoop analytics paradigm. There are many ways to use R with these platforms. In this guide, we will outline the most common practices and provide code examples to help get you started.\r\n\r\nRegardless of the size of data, the \"last mile\" of operationalizing data-driven discoveries has traditionally often been an area of challenge.  With the advent of lightweight web frameworks for data scientists such as [Shiny](http://shiny.rstudio.com/) and highly automated hosting platforms such as [Cloud Foundry (CF)](https://www.cloudfoundry.org/), the effort involved in developing data-driven smart apps for end users has been reduced vastly.  In this document, we will also outline some guidelines to help you get started on pushing your Shiny apps to the cloud.  \r\n\r\nOfficial documentation can be found here:\r\n\r\n* GPDB\r\n  * [Product Page](http://pivotal.io/big-data/pivotal-greenplum)\r\n  * [Documentation](http://gpdb.docs.pivotal.io/index.html)\r\n  * [Installation guide](http://gpdb.docs.pivotal.io/4320/pdf/GPDB43InstallGuide.pdf)\r\n  * [Administrator guide](http://gpdb.docs.pivotal.io/4320/pdf/GPDB43AdminGuide.pdf)\r\n* Apache HAWQ\r\n  * [Product Page](http://hawq.incubator.apache.org)\r\n  * [Documentation](http://hawq.docs.pivotal.io/index.html)\r\n  * [Installation guide](http://hawq.docs.pivotal.io/docs-hawq/topics/HAWQInstallationandUpgrade.html)\r\n  * [Administrator guide](http://hawq.docs.pivotal.io/docs-hawq/topics/HAWQAdministration.html)\r\n  * [Github Repository](https://github.com/apache/incubator-hawq) \r\n* Cloud Foundry\r\n  * [Product Page](https://www.cloudfoundry.org/)\r\n  * [Documentation](http://docs.cloudfoundry.org/)\r\n  * [Installing PCF-Dev (i.e. CF in a VM)](https://docs.pivotal.io/pcf-dev/)\r\n  * [Github Repository](https://github.com/cloudfoundry) \r\n  * [R Buildpack](https://github.com/wjjung317/heroku-buildpack-r)\r\n  \r\nNote that this Github page is intended as a guide for **practitioners** and **should not** be considered official documentation. The intention is to give pragmatic tips on how to use the GPDB, HAWQ, and Cloud Foundry with the R statistical programming environment.  \r\n\r\n# <a name=\"plr\"/> PL/R on Greenplum & HAWQ\r\n\r\n## <a name=\"plr_gettingstarted\"/> Getting Started\r\n\r\n### <a name=\"plr_arch\"/> PL/R Architecture\r\n\r\n![alt text](https://github.com/zimmeee/gp-r/blob/master/figures/PLR_GPDB_Architecture.png?raw=true \"Distributed PL/R architecture on GPDB\")\r\n\r\nPL/R provides a connection from the database to R -- which is running on every segment of the Greenplum instance -- to allow you to write procedural functions in R. In this setup R is not a client application that runs on the desktop like pgadmin. It runs on each segment of the server.\r\n\r\n### Datasets for Examples\r\n\r\nThis guide contains code examples interspersed with explanations in natural language. You are encouraged to follow along with the examples, most of which will use the `abalone` [dataset](http://archive.ics.uci.edu/ml/datasets/Abalone) from the UC Irvine [Machine Learning Repository](http://archive.ics.uci.edu/ml/index.html).\r\n\r\nTo get started, download the data onto the file system of the GPDB/HAWQ host machine, and note the path: \r\n```\r\nwget http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\r\npwd\r\n```\r\n\r\nNext, create a table in GPDB/HAWQ to store the abalone data. Note that `/path/to/data` is the path returned by `pwd` in the previous line of code. \r\n\r\n```\r\nDROP TABLE IF EXISTS abalone;\r\nCREATE TABLE abalone (sex text, length float8, diameter float8, height float8, whole_weight float8, shucked_weight float8, viscera_weight float8, shell_weight float8, rings float8) \r\nDISTRIBUTED RANDOMLY;\r\nCOPY abalone FROM '/path/to/data/abalone.data' WITH CSV;\r\n```\r\n\r\nYou should now have a table in the `public` schema of your database containing 4177 rows.\r\n\r\n```\r\nuser# select count(*) from abalone;\r\n count \r\n-------\r\n  4177\r\n(1 row)\r\n```\r\n\r\n### <a name=\"installation\"/> Installation\r\n\r\n\r\n#### Install and verify PL/R\r\n\r\nPivotal Engineering ships its own version of PL/R as a gppkg. You will not be able to download the source from Joe Conway's website\r\nand compile it against the postgres headers supplied by Pivotal. Although HAWQ and Greenplum are based on Postgres 8.2, the source codes have diverged enough that your compilation of PL/R source (for Postgres 8.2) with Pivotal-supplied postgres headers will not be successful.\r\nPlease contact support to obtain the gppkg for PL/R for your installation (internally, it can also be downloaded from Pivotal Network). Once obtained, the gppkg for PL/R can be installed by following the steps below:\r\n\r\nGppkg command can be used to install PL/R on all segments.\r\n\r\n```\r\ngppkg --install plr-1.0-rhel5-x86_64.gppkg\r\n```\r\n\r\nThis will install both PL/R and R as well.\r\nYou will find a folder `/usr/local/greenplum-db/ext/R-2.13/` upon the successful installation of the previous command.\r\n\r\nYou should see a trace like the following for each segment\r\n\r\n```\r\nbash-4.1$ gppkg --install plr-1.0-rhel5-x86_64.gppkg\r\n20130524:10:56:17:007456 gppkg:agni_centos:gpadmin-[INFO]:-Starting gppkg with args: --install plr-1.0-rhel5-x86_64.gppkg\r\n20130524:10:56:18:007456 gppkg:agni_centos:gpadmin-[INFO]:-Installing package plr-1.0-rhel5-x86_64.gppkg\r\n20130524:10:56:18:007456 gppkg:agni_centos:gpadmin-[INFO]:-Validating rpm installation cmdStr='rpm --test -i /usr/local/greenplum-db-4.2.2.4/.tmp/plr-1.0-1.x86_64.rpm /usr/local/greenplum-db-4.2.2.4/.tmp/R-2.13.0-1.x86_64.rpm --dbpath /usr/local/greenplum-db-4.2.2.4/share/packages/database --prefix /usr/local/greenplum-db-4.2.2.4'\r\n20130524:10:56:18:007456 gppkg:agni_centos:gpadmin-[INFO]:-Installing plr-1.0-rhel5-x86_64.gppkg locally\r\n20130524:10:56:19:007456 gppkg:agni_centos:gpadmin-[INFO]:-Validating rpm installation cmdStr='rpm --test -i /usr/local/greenplum-db-4.2.2.4/.tmp/plr-1.0-1.x86_64.rpm /usr/local/greenplum-db-4.2.2.4/.tmp/R-2.13.0-1.x86_64.rpm --dbpath /usr/local/greenplum-db-4.2.2.4/share/packages/database --prefix /usr/local/greenplum-db-4.2.2.4'\r\n20130524:10:56:19:007456 gppkg:agni_centos:gpadmin-[INFO]:-Installing rpms cmdStr='rpm -i /usr/local/greenplum-db-4.2.2.4/.tmp/plr-1.0-1.x86_64.rpm /usr/local/greenplum-db-4.2.2.4/.tmp/R-2.13.0-1.x86_64.rpm --dbpath /usr/local/greenplum-db-4.2.2.4/share/packages/database --prefix=/usr/local/greenplum-db-4.2.2.4'\r\n20130524:10:56:20:007456 gppkg:agni_centos:gpadmin-[INFO]:-Completed local installation of plr-1.0-rhel5-x86_64.gppkg.\r\n20130524:10:56:20:007456 gppkg:agni_centos:gpadmin-[INFO]:-Please source your $GPHOME/greenplum_path.sh file and restart the database.\r\nYou can enable PL/R by running createlang plr -d mydatabase.\r\n20130524:10:56:20:007456 gppkg:agni_centos:gpadmin-[INFO]:-plr-1.0-rhel5-x86_64.gppkg successfully installed.\r\n```\r\n\r\nThe installation can be verified by checking for the existence of the PL/R shared object in `/usr/local/greenplum-db/lib/postgresql/plr.so`\r\n\r\nNow you'll have to source /usr/local/greenplum-db/greenplum_path.sh and restart GPDB for changes to the `LD_LIBRARY_PATH` environment variable to take effect.\r\nFollowing the installation you'll see that the environment variable `R_HOME` has been set on all segments.\r\n```\r\n[gpadmin@mdw ~]$ echo $R_HOME\r\n/usr/local/greenplum-db/./ext/R-2.13.0/lib64/R\r\n[gpadmin@mdw ~]$ \r\n```\r\n\r\nYou can then install PL/R on your database by running\r\n\r\n```\r\nCREATE LANGUAGE PLR;\r\n```\r\n\r\nYou may also install it on the template1 database to ensure every newly created database automatically has PL/R installed in it.\r\n\r\n\r\n### <a name=\"permissions\"/> Note on Permissions\r\nR is an [untrusted language](http://www.postgresql.org/docs/current/interactive/catalog-pg-language.html). Only superusers can create functions in untrusted languages. A discussion as to whether granting super user privileges on the database is acceptable needs to be an explicit step in selecting PL/R for your analytics project. \r\n\r\nThis is what happens when you try to create a PL/R function when you aren't a superuser:\r\n\r\n``` \r\nERROR:  permission denied for language plr\r\n\r\n********** Error **********\r\n\r\nERROR: permission denied for language plr\r\nSQL state: 42501\r\n```\r\n\r\nYou do not need superuser privileges to EXECUTE a PL/R function, only to CREATE a PL/R function. Thus, non-superusers *can run* a PL/R function that was created by a superuser. In the GP Admin Guide there is a section entitled 'Managing Object Privileges' which outlines how to grant privileges to other roles for executing untrusted languages. \r\n\r\nGRANT USAGE privilege to the account \r\nhttp://lists.pgfoundry.org/pipermail/plr-general/2010-August/000441.html\r\n\r\n## <a name=\"packages\"/> Leveraging R Packages\r\nThe trick to installing R packages in a distributed Greenplum environment is that each segment has it's own R instance running and thus each segment needs its own version of all of the required packages. At a high-level, the steps for installing R packages on a Greenplum instance are:\r\n\r\n1. Get the package tars from CRAN (`wget`)\r\n2. Copy the tar to all the segments on the DCA (`gpscp`)\r\n3. Install the package (`gpssh`, then `R CMD INSTALL`)\r\n\r\n\r\nNote that any time you install a new R library/package using:\r\n\r\n```\r\nR CMD INSTALL <package name>\r\n```\r\n\r\nThe resulting shared object (.so file) of the library\r\nshould be generated in `/usr/local/greenplum-db/ext/R-2.13.0/lib64/R/library/<library_name>`\r\n\r\n### <a name=\"plr_packages_check\"/> Checking R Package Availability\r\n\r\nR packages are the special sauce of R. This section explains how to check whether a package is installed and how to install new packages. The simplest way to check if the requires R packages are available for PL/R is to `gpssh` into all the nodes and test if you are able to find the version of the required package. All the nodes\r\nshould return the correct version of the package, if the installation was successful.\r\n\r\n```\r\ngpssh -f all_hosts\r\n=> echo \"packageVersion('rpart')\" | R --no-save\r\n\r\n[sdw11] > packageVersion('rpart')\r\n[sdw11] [1] ‘3.1.49’\r\n[ sdw9] > packageVersion('rpart')\r\n[ sdw9] [1] ‘3.1.49’\r\n.\r\n.\r\n.\r\n```\r\n\r\nIf the package is unavailable, the above code will error out. In the snippet below, we check for the version of the `HMM` package\r\nin our installation. As there is no such package installed, the command will not execute successfully.\r\n\r\n```\r\ngpssh -f all_hosts\r\n=> echo \"packageVersion('hmm')\" | R --no-save\r\n[ sdw2] > packageVersion('hmm')\r\n[ sdw2] Error in packageVersion(\"hmm\") : package ‘hmm’ not found\r\n[ sdw2] Execution halted\r\n[ sdw3] > packageVersion('hmm')\r\n[ sdw3] Error in packageVersion(\"hmm\") : package ‘hmm’ not found\r\n[ sdw3] Execution halted\r\n\r\n```\r\n\r\nIf you do not have access to SSH into the GPDB or you prefer to only deal with UDFs to tell you if a PL/R package is present or absent, then you can write UDFs like the following:\r\n\r\nA simple test if a package can be loaded can be done by this function:\r\n```SQL\r\nCREATE OR REPLACE FUNCTION R_test_require(fname text)\r\nRETURNS boolean AS\r\n$BODY$\r\n    return(require(fname,character.only=T))\r\n$BODY$\r\nLANGUAGE 'plr';\r\n```\r\n\r\nIf you want to check for a package called 'rpart', you would do\r\n```SQL\r\nSELECT R_test_require('rpart');\r\n```\r\n\r\nAnd it will return `TRUE` if the package could be loaded and `FALSE` if it couldn't. However, this only works on the node that you are currently logged on to.\r\n\r\nTo test the R installations on all nodes you would first create a dummy table with a series of integers that will be stored on different nodes in GPDB, like this:\r\n\r\n```SQL\r\nDROP TABLE IF EXISTS simple_series;\r\nCREATE TABLE simple_series AS (SELECT generate_series(0,1000) AS id);\r\n```\r\n\r\nAlso, since we want to know which host we are on we create a function to tell us:\r\n\r\n```SQL\r\nCREATE OR REPLACE FUNCTION R_return_host()\r\nRETURNS text AS\r\n$BODY$\r\n  return(system(\"hostname\",intern=T))\r\n$BODY$\r\nLANGUAGE 'plr';\r\n```\r\n\r\nNow we can check for each id (ids are stored on different nodes) if rpart is installed like this:\r\n```SQL\r\nDROP TABLE IF EXISTS result_nodes;\r\nCREATE TABLE result_nodes AS \r\n    (SELECT id, R_return_host() AS hostname, R_test_require('rpart') AS result \r\n    FROM simple_series group by id); \r\n```\r\n\r\n`result_nodes` is a table that contains for every id, the host that it is stored on as `hostname`, and the result of `R_test_require` as result. Since we only want to know for every host once, we group by `hostname` like this:\r\n\r\n```SQL\r\nselect hostname, bool_and(result) AS host_result \r\nFROM result_nodes \r\nGROUP BY hostname \r\nORDER BY hostname;\r\n```\r\n\r\nFor a hostname where `R_test_require` returned true for all ids, the value in the column `host_result` will be true. If on a certain host the package couldn't be loaded, `host_result` will be false.\r\n\r\n### <a name=\"plr_packages_install\"/> Installing R Packages\r\n\r\nBefore installing the packages for PL/R ensure that you are referring to the right R binary in your PATH and also ensure that the environment variable `R_HOME` is referring to the right location where you installed R. These paths should be identical on all master and segment nodes.\r\n\r\nSome users have a separate stand-alone installation of R on just the master node. If this is the case with your installation, ensure that this does not conflict with installation you need for PL/R to run on multiple segments.\r\n\r\nFor a given R package, identify all dependent R packages and the package URLs.  This can be found by selecting the given package from the following navigation page: \r\n`http://cran.r-project.org/web/packages/available_packages_by_name.html`\r\n\r\nFrom the page for the `arm` library, it can be seen that this library requires the following R libraries: `Matrix`, `lattice`, `lme4`, `R2WinBUGS`, `coda`, `abind`, `foreign`, `MASS`\r\n\r\nFrom the command line, use wget to download the required packages' `tar.gz` files to the master node:\r\n\r\n```\r\nwget http://cran.r-project.org/src/contrib/arm_1.5-03.tar.gz\r\nwget http://cran.r-project.org/src/contrib/Archive/Matrix/Matrix_1.0-1.tar.gz\r\nwget http://cran.r-project.org/src/contrib/Archive/lattice/lattice_0.19-33.tar.gz\r\nwget http://cran.r-project.org/src/contrib/lme4_0.999375-42.tar.gz\r\nwget http://cran.r-project.org/src/contrib/R2WinBUGS_2.1-18.tar.gz\r\nwget http://cran.r-project.org/src/contrib/coda_0.14-7.tar.gz\r\nwget http://cran.r-project.org/src/contrib/abind_1.4-0.tar.gz\r\nwget http://cran.r-project.org/src/contrib/foreign_0.8-49.tar.gz\r\nwget http://cran.r-project.org/src/contrib/MASS_7.3-17.tar.gz\r\n```\r\n\r\nUsing `gpscp` and the hostname file, copy the `tar.gz` files to the same directory on all nodes of the GPDB/HAWQ cluster.  Note that this may require root access. (note: location and name of host file may be different. On our DCA its /home/gpadmin/all_hosts)\r\n\r\n```\r\ngpscp -f /home/gpadmin/all_hosts lattice_0.19-33.tar.gz =:/home/gpadmin \r\ngpscp -f /home/gpadmin/all_hosts Matrix_1.0-1.tar.gz =:/home/gpadmin \r\ngpscp -f /home/gpadmin/all_hosts abind_1.4-0.tar.gz =:/home/gpadmin \r\ngpscp -f /home/gpadmin/all_hosts coda_0.14-7.tar.gz =:/home/gpadmin \r\ngpscp -f /home/gpadmin/all_hosts R2WinBUGS_2.1-18.tar.gz =:/home/gpadmin \r\ngpscp -f /home/gpadmin/all_hosts lme4_0.999375-42.tar.gz =:/home/gpadmin \r\ngpscp -f /home/gpadmin/all_hosts MASS_7.3-17.tar.gz =:/home/gpadmin\r\ngpscp -f /home/gpadmin/all_hosts arm_1.5-03.tar.gz =:/home/gpadmin\r\n```\r\n\r\n`gpssh` into all segments (`gpssh -f /home/gpadmin/all_hosts`).  Install the packages from the command prompt using the `R CMD INSTALL` command.  Note that this may require root access \r\n\r\n```\r\nR CMD INSTALL lattice_0.19-33.tar.gz Matrix_1.0-1.tar.gz abind_1.4-0.tar.gz coda_0.14-7.tar.gz R2WinBUGS_2.1-18.tar.gz lme4_0.999375-42.tar.gz MASS_7.3-17.tar.gz arm_1.5-03.tar.gz\r\n```\r\n\r\nCheck that the newly installed package is listed under the `$R_HOME/library` directory on all the segments (convenient to use `gpssh` here as well).\r\n\r\n### <a name=\"plr_packages_versions\"/> Note on R Package Versions & Dependencies\r\nSometimes the current version of a package has dependencies on an earlier version of R. If this happens, you might get an error message like:\r\n\r\n```\r\nIn getDependencies(pkgs, dependencies, available, lib) :\r\n  package ‘matrix’ is not available (for R version 2.13.0)\r\n```\r\n\r\nFortunately, there are older versions of most packages available in the CRAN archive. One heuristic we’ve found useful is to look at the release date of the R version installed on the machine. At the time of writing, it is v2.13 on our analytics DCA, which was released on 13-Apr-2011 (http://cran.r-project.org/src/base/R-2/). Armed with this date, go to the archive folder for the package you are installing and find the version that was released immediately prior to that date. For instance, the v1.5.3 of the package `glmnet` was released on 01-Mar-2011 and should be compatible with R v2.13 (http://cran.r-project.org/src/contrib/Archive/glmnet/ ) and download that version. This manual heuristic works reasonably well for finding compatible package versions. \r\n\r\n## <a name=\"bestpractices\"/> Usage & Best Practices\r\nHere we outline workflows that have worked well for us in past experiences using R on GPDB & HAWQ.  \r\n\r\nOne overarching theme for PL/R on GPDB/HAWQ is that it is best suited in scenarios where the problem that you want to solve is one that is embarrassingly parallelizable. A simple way to think about PL/R is that it is provides functionality akin to MapReduce or R’s apply family of functions – with the added bonus of leveraging GPDB/HAWQ native architecture to execute each mapper. In other words, it provides a nice framework for you to run parallelized `for` loops containing R jobs in GPDB/HAWQ.  We focus our description of best practices around this theme.\r\n\r\n  * [Make a plan](#makeplan)\r\n  * [Data prep](#dataprep)\r\n  * [Return types](#returntypes)\r\n  * [PL/R UDF Definition](#udf)\r\n  * [PL/R Execution](#execution)\r\n  * [Persisting R Models in GPDB & HAWQ](#persistence)\r\n  * [Verifying Parallelization](#parallelization)\r\n\r\n### <a name=\"makeplan\"/> Make a Plan\r\nBefore doing anything, ask yourself whether the problem you are solving is explicitly parallelizable.  If so, identify what you’d like to parallelize by.  In other words, what is the index of your for loop?  This will play a large role in determining how you will prepare your data and build your PL/R function.\r\n\r\nUsing the abalone data as an example, let’s suppose you were interested in building a separate, completely independent model for each sex of abalone in the dataset.  Under this scenario, it’s clear that it would then make sense to parallelize by the abalone’s sex.  \r\n\r\n### <a name=\"dataprep\"/> Data Preparation\r\nIt’s often good practice to build another version of your table, dimensioned by the field by which you’d like to parallelize.  Let’s call this field the parallelization index for shorthand.  You essentially want to build a table where each row contains all the data for each value of the parallelization index.  This is done by array aggregation.  Using the SQL `array_agg()` function, aggregate all of the records for each unique value of the parallelization index into a single row.  \r\n\r\nAn example will make this more clear.  Let’s take a look at our raw abalone table:\r\n```SQL\r\nSELECT * FROM abalone LIMIT 3;\r\n sex | length | diameter | height | whole_weight | shucked_weight | viscera_weight | shell_weight | rings \r\n-----+--------+----------+--------+--------------+----------------+----------------+--------------+-------\r\n M   |  0.405 |     0.31 |    0.1 |        0.385 |          0.173 |         0.0915 |         0.11 |     7\r\n M   |  0.425 |     0.35 |  0.105 |        0.393 |           0.13 |          0.063 |        0.165 |     9\r\n I   |  0.315 |    0.245 |  0.085 |       0.1435 |          0.053 |         0.0475 |         0.05 |     8\r\n(3 rows)\r\n```\r\nLet’s suppose that the end goal is to build a separate regression model for each sex with shucked_weight as the response variable and rings, diameter as explanatory variables.  Thinking ahead to this end goal, you would then create another version of the data table by:\r\n\r\n1. Array aggregating each variable of interest,\r\n2. Grouping by the parallelization index, and\r\n3. Distributing by the parallelization index\r\n\r\nTo continue our example:\r\n\r\n```SQL\r\nDROP TABLE IF EXISTS abalone_array;\r\nCREATE TABLE abalone_array AS SELECT \r\nsex::text\r\n, array_agg(shucked_weight::float8) as s_weight\r\n, array_agg(rings::float8) as rings\r\n, array_agg(diameter::float8) as diameter \r\nFROM abalone \r\nGROUP BY sex \r\nDISTRIBUTED BY (sex);\r\n```\r\nThe raw table is array aggregated into a table with rows equal to the number of unique values of the parallelization index.  For this specific example, there are three unique values of sex in the abalone data, and thus there are three rows in the abalone_array table.   \r\n\r\n### <a name=\"returntypes\"/> Return Types\r\nAs described in the Data Types section, it’s often difficult to read SQL arrays, and it's not possible to have SQL arrays containing both text and numeric entries.  For this reason, our best practice is to use custom composite types as return types for PL/R functions in Greenplum.  \r\n\r\nIt’s useful to think ahead and identify what the final output of your PL/R function will be.  In the case of our example, since we are running regressions, let’s suppose we want to return information that looks a lot like R’s `summary.lm()` function.  In particular, we are interested in getting back a table with each explanatory variable’s name, the coefficient estimate, standard error, t-statistic, and p-value.  With this in mind, we build a custom composite type as a template for the output we intend to get back from our PL/R function.  \r\n```SQL\r\nDROP TYPE IF EXISTS lm_abalone_type CASCADE;\r\nCREATE TYPE lm_abalone_type AS (\r\nVariable text, Coef_Est float, Std_Error float, T_Stat float, P_Value float); \r\n```\r\n\r\n### <a name=\"udf\"/> PL/R UDF Definition\r\nNow that we’ve defined the structure of our input and output values, we can go ahead and tell GPDB/HAWQ and R what we want to do with this data.  We are now ready to define our PL/R function. \r\n\r\nA couple of helpful rules to follow here:\r\n* Each argument of the PL/R function and its specified data type should correspond to a column that exists in the array aggregated table that was created in the Data Prep step\r\n* The return data type of the PL/R function should be a SETOF the composite type that was created in the Return Types step\r\n\r\nContinuing our example using the abalone data, we define the following PL/R function:\r\n\r\n```SQL\r\nCREATE OR REPLACE FUNCTION lm_abalone_plr(s_weight float8[], rings float8[], diameter float8[]) \r\nRETURNS SETOF lm_abalone_type AS \r\n$$ \r\n    m1<- lm(s_weight~rings+diameter)\r\n    m1_s<- summary(m1)$coef\r\n    temp_m1<- data.frame(rownames(m1_s), m1_s)\r\n    return(temp_m1)\r\n$$ \r\nLANGUAGE 'plr';\r\n```\r\n\r\n### <a name=\"execution\"/> PL/R Execution\r\nWe then execute the PL/R function by specifying the parallelization index and the function call in the SELECT statement.  \r\n\r\nTo conclude our example, we run the following SELECT statement to run 3 separate regression models; one model for each sex.  Under this scenario, execution is parallelized by the abalone’s sex:\r\n```SQL\r\nSELECT  sex, (lm_abalone_plr(s_weight,rings,diameter)).* FROM abalone_array;\r\n sex |  variable   |       coef_est       |      std_error       |       t_stat       |        p_value        \r\n -----+-------------+----------------------+----------------------+--------------------+----------------------- \r\n F   | (Intercept) |   -0.617050922097655 |   0.0169416168113397 |   -36.422198009144 | 6.03016903934925e-201 \r\n F   | rings       | -0.00956233525043721 | 0.000835808125948978 |  -11.4408258947951 |  5.96598342834597e-29 \r\n F   | diameter    |     2.57219713416591 |   0.0365667203043869 |    70.342571407951 |                     0 \r\n M   | (Intercept) |   -0.534293488484019 |   0.0148876715438078 |  -35.8883178549332 | 5.42293200035969e-205 \r\n M   | rings       |  -0.0101856670676353 |  0.00096233015174409 |  -10.5843790191704 |  2.59455668009866e-25 \r\n M   | diameter    |     2.45006792350753 |   0.0345072752341834 |   71.0014890158715 |                     0\r\n I   | (Intercept) |   -0.236131314300337 |  0.00601596875673268 |  -39.2507547576729 | 6.73787958361764e-225\r\n I   | rings       | -0.00046870969168018 | 0.000850383676525165 | -0.551174375307179 |     0.581606099530735\r\n I   | diameter    |     1.31967087153234 |   0.0242402717496186 |   54.4412573078149 |                     0\r\n(9 rows)\r\n\r\n```\r\n\r\n### <a name=\"persistence\"/> Persisting R Models in the Database\r\nOne benefit of using PL/R on a parallelized platform like GPDB/HAWQ is the ability to perform scoring in parallel across all the segments.\r\nIf you've trained a GLM model for instance, you could save a serialized version of this model in a database table and de-serialize it when needed and use it for scoring.\r\n\r\nTypically the models are built once or are trained periodically depending on what the application may be, but the scoring may have to happen in real-time as new data becomes available.\r\nIf the data to be scored is stored in a table distributed across the segments on GPDB/HAWQ, then by ensuring the trained models are also distributed across the same segments, we can achieve parallel scoring through PL/R.\r\n\r\nThe simplest approach would be to serialize the entire model into a byte array and store it in a table, although not all parameters of the R model are required for scoring. For example, for linear or logistic regression we only need the coefficients of the features to perform scoring. Advanced users should be able to extract only the relevant parameters from the model and serialize them into a byte array on a table. This will improve scoring speed as the segment nodes won't have to de-serialize large byte arrays. Another optimization that will speed up scoring will be to pre-load the models into memory on the segment nodes - so that models are not de-serialized for every PL/R function call. In both these cases the user will have to write additional logic beside the scoring itself, for the optimization.\r\n\r\nIn the sample code shown below we demonstrate some of these optimizations. This guide is work in progress and in the upcoming versions we will include more examples to optimize the scoring function.\r\n\r\nFirst we'll define a custom record type to hold the results from a GLM model. This is equivalent to the summary() function in R.\r\n\r\n```SQL \r\n\tDROP TYPE IF EXISTS gpdemo.glm_result_type CASCADE;\r\n\tCREATE TYPE gpdemo.glm_result_type \r\n\tAS \r\n\t(\r\n\t\tparams text, \r\n\t\testimate float, \r\n\t\tstd_Error float, \r\n\t\tz_value float, \r\n\t\tpr_gr_z float\r\n\t);\r\n```\r\n\r\nHere is a PL/R function that demonstrates how a trained GLM model can be serialized as a byte array. The sample table `patient_history_train` is included in the data folder of this repository.\r\n\r\n```SQL\r\n\tDROP FUNCTION IF EXISTS gpdemo.mdl_save_demo();\r\n\tCREATE FUNCTION gpdemo.mdl_save_demo() \r\n        RETURNS bytea \r\n        AS\r\n\t$$\r\n\t     #Read the previously created patient_history training set\r\n\t     dataset <- pg.spi.exec('select * from gpdemo.patient_history_train');\r\n\r\n\t     # Use the subset function to select a subset of the columns\r\n             # Indices 2:6 are age, gender, race, marital status and bmi\r\n             # Indices 14:20 are med_cond1 to med_cond7\r\n             # Index 26 is the label 'infection cost'\r\n\t     ds = subset(dataset,select=c(2:6,14:20, 26))\r\n\r\n\t     #Define text  columns to be factor types\r\n\t     #These include gender, race, marital_status\t     \r\n\t     ds$gender = as.factor(ds$gender)\r\n\t     ds$race = as.factor(ds$race)\r\n\t     ds$marital_status = as.factor(ds$marital_status)\r\n\r\n\t     #Fit a GLM\r\n\t     mdl = glm(formula = infection_cost ~ age +\r\n\t\t\t\t  gender +\r\n\t\t\t\t  race +\r\n\t\t\t\t  marital_status +\r\n\t\t\t\t  bmi +\r\n\t\t\t\t  med_cond1 +\r\n\t\t\t\t  med_cond2 +\r\n\t\t\t\t  med_cond3 +\r\n\t\t\t\t  med_cond4 +\r\n\t\t\t\t  med_cond5 +\r\n\t\t\t\t  med_cond6 +\r\n\t\t\t\t  med_cond7 \r\n\t\t\t, family = gaussian, data=ds)\r\n\t     #The model is serialized and returned as a bytearray\r\n\t     return (serialize(mdl,NULL))\r\n\t$$\r\n\tLANGUAGE 'plr';\r\n```\r\n\r\nHere is a PL/R function to read a serialized PL/R model examine it's parameters.\r\n\r\n```SQL\r\n\tDROP FUNCTION IF EXISTS gpdemo.mdl_load_demo(bytea);\r\n\tCREATE FUNCTION gpdemo.mdl_load_demo(mdl bytea) \r\n        RETURNS setof gpdemo.glm_result_type \r\n        AS\r\n\t$$\r\n\t     #R-code goes here.\r\n\t     mdl <- unserialize(mdl)\r\n\t     cf <- coef(summary(mdl))\r\n\t     rows = dimnames(cf)[1]\r\n\t     #Create a data frame and pass that as a result\r\n\t     result = data.frame(params=rows[[1]],estimate=cf[,1],error=cf[,2],z_val=cf[,3],pr_z=cf[,4])\r\n\t     return (result)\r\n\t$$\r\n\tLANGUAGE 'plr';\r\n```\r\n\r\nThe function can be invoked like so:\r\n\r\n```SQL\r\n\tselect (t).params, \r\n\t       (t).estimate,\r\n\t       (t).std_Error, \r\n\t       (t).z_value float, \r\n\t       (t).pr_gr_z \r\n\tfrom \r\n\t(\r\n\t       -- The column 't' is of glm_result_type that we defined in step 3s.\r\n\t       select mdl_load_demo(model) as t \r\n\t       from mdls\r\n\t) q ;\r\n```\r\n\r\nHere is the PL/R function which demonstrate parallel scoring using the GLM model we trained in the example above.\r\n\r\n```SQL\r\n\tDROP FUNCTION IF EXISTS gpdemo.mdl_score_demo( bytea, \r\n\t\t\t\t\t\t\tinteger,\r\n\t\t\t\t\t\t\ttext,\r\n\t\t\t\t\t\t\ttext,\r\n\t\t\t\t\t\t\ttext,\r\n\t\t\t\t\t\t\tdouble precision,\r\n\t\t\t\t\t\t\tinteger,\r\n\t\t\t\t\t\t\tinteger,\r\n\t\t\t\t\t\t\tinteger,\r\n\t\t\t\t\t\t\tinteger,\r\n\t\t\t\t\t\t\tinteger,\r\n\t\t\t\t\t\t\tinteger,\r\n\t\t\t\t\t\t\tinteger\r\n\t\t\t\t\t\t      );\r\n\tCREATE FUNCTION gpdemo.mdl_score_demo( mdl bytea, \r\n\t\t\t\t\t\tage integer,\r\n\t\t\t\t\t\tgender text,\r\n\t\t\t\t\t\trace text,\r\n\t\t\t\t\t\tmarital_status text,\r\n\t\t\t\t\t\tbmi double precision,\r\n\t\t\t\t\t\tmed_cond1 integer,\r\n\t\t\t\t\t\tmed_cond2 integer,\r\n\t\t\t\t\t\tmed_cond3 integer,\r\n\t\t\t\t\t\tmed_cond4 integer,\r\n\t\t\t\t\t\tmed_cond5 integer,\r\n\t\t\t\t\t\tmed_cond6 integer,\r\n\t\t\t\t\t\tmed_cond7 integer\t\r\n\t\t\t\t\t      ) \r\n\tRETURNS numeric AS\r\n\t$$\r\n\t     if (pg.state.firstpass == TRUE) {\r\n\t     \t#Unserialize the model (i.e reconstruct it from its binary form).\r\n\t        assign(\"gp_plr_mdl_score\", unserialize(mdl) ,env=.GlobalEnv)\r\n\t        assign(\"pg.state.firstpass\", FALSE, env=.GlobalEnv)\r\n\t     }\r\n\r\n\r\n\t     #Read the test set from the previously created table  \r\n\t     test_set <- data.frame(\r\n\t\t\t\t\tage = age,\r\n\t\t\t\t\tgender = gender,\r\n\t\t\t\t\trace = race,\r\n\t\t\t\t\tmarital_status = marital_status, \r\n\t\t\t\t\tbmi =  bmi,\r\n\t\t\t\t\tmed_cond1 =  med_cond1,\r\n\t\t\t\t\tmed_cond2 =  med_cond2,\r\n\t\t\t\t\tmed_cond3 =  med_cond3,\r\n\t\t\t\t\tmed_cond4 =  med_cond4,\r\n\t\t\t\t\tmed_cond5 =  med_cond5,\r\n\t\t\t\t\tmed_cond6 =  med_cond6,\r\n\t\t\t\t\tmed_cond7 =  med_cond7  \t\r\n\t\t\t            );\r\n\t     #Perform prediction\r\n\t     pred <- predict(gp_plr_mdl_score, newdata=test_set, type=\"response\"); \r\n\r\n\t     return (pred)\r\n\t$$\r\n\tLANGUAGE 'plr';\r\n\r\n```\r\n\r\nYou can also score a whole array:\r\n```SQL\r\n\tDROP FUNCTION IF EXISTS gpdemo.mdl_score_demo( bytea, \r\n\t\t\t\t\t\t\tinteger[],\r\n\t\t\t\t\t\t\ttext[],\r\n\t\t\t\t\t\t\ttext[],\r\n\t\t\t\t\t\t\ttext[],\r\n\t\t\t\t\t\t\tdouble precision[],\r\n\t\t\t\t\t\t\tinteger[],\r\n\t\t\t\t\t\t\tinteger[],\r\n\t\t\t\t\t\t\tinteger[],\r\n\t\t\t\t\t\t\tinteger[],\r\n\t\t\t\t\t\t\tinteger[],\r\n\t\t\t\t\t\t\tinteger[],\r\n\t\t\t\t\t\t\tinteger[] \r\n\t\t\t\t\t\t      );\r\n\tCREATE FUNCTION gpdemo.mdl_score_demo( mdl bytea, \r\n\t\t\t\t\t\tage integer[],\r\n\t\t\t\t\t\tgender text[],\r\n\t\t\t\t\t\trace text[],\r\n\t\t\t\t\t\tmarital_status text[],\r\n\t\t\t\t\t\tbmi double precision[],\r\n\t\t\t\t\t\tmed_cond1 integer[],\r\n\t\t\t\t\t\tmed_cond2 integer[],\r\n\t\t\t\t\t\tmed_cond3 integer[],\r\n\t\t\t\t\t\tmed_cond4 integer[],\r\n\t\t\t\t\t\tmed_cond5 integer[],\r\n\t\t\t\t\t\tmed_cond6 integer[],\r\n\t\t\t\t\t\tmed_cond7 integer[] \r\n\t\t\t\t\t      ) \r\n\tRETURNS numeric[]\r\n    IMMUTABLE\r\n    AS\r\n\t$$\r\n\t    gp_plr_mdl_score <- unserialize(mdl)\r\n\r\n#Read the test set from the previously created table\r\n\t     test_set <- data.frame(\r\n\t\t\t\t\tage = age,\r\n\t\t\t\t\tgender = gender,\r\n\t\t\t\t\trace = race,\r\n\t\t\t\t\tmarital_status = marital_status, \r\n\t\t\t\t\tbmi =  bmi,\r\n\t\t\t\t\tmed_cond1 =  med_cond1,\r\n\t\t\t\t\tmed_cond2 =  med_cond2,\r\n\t\t\t\t\tmed_cond3 =  med_cond3,\r\n\t\t\t\t\tmed_cond4 =  med_cond4,\r\n\t\t\t\t\tmed_cond5 =  med_cond5,\r\n\t\t\t\t\tmed_cond6 =  med_cond6,\r\n\t\t\t\t\tmed_cond7 =  med_cond7  \t\r\n\t\t\t            );\r\n\t     #Perform prediction\r\n\t     pred <- predict(gp_plr_mdl_score, newdata=test_set, type=\"response\"); \r\n\r\n\t     return (pred)\r\n\t$$\r\n\tLANGUAGE 'plr';\r\n\r\n```\r\n\r\nThe training, loading and scoring functions can be invoked from SQL like so :\r\n\r\n```SQL\r\n\t-- Compute R square (coefficient of determination)\r\n\t-- R_square = (1 - SS_err/SS_tot)\r\n\tselect 'PL/R glm model '::text as model, \r\n\t       (1.0 - sum(ss_err)*1.0/sum(ss_tot)) as R_square\r\n\tfrom\r\n\t(\r\n\t\tselect instance_num, \r\n\t\t(infection_cost_actual - (select avg(infection_cost) from gpdemo.patient_history_test) )^2.0 as ss_tot,\r\n\t\t(infection_cost_actual -  infection_cost_predicted)^2.0 as ss_err,\t\t\r\n\t\t1 as cnt\r\n\t\tfrom\r\n\t\t(\r\n\t\t\t-- Show actual vs predicted values for the infection cost\r\n\t\t\tselect row_number() over (order by random()) as instance_num, \r\n\t\t\t\tinfection_cost as infection_cost_actual,\r\n\t\t\t\tgpdemo.mdl_score_demo ( mdls.model, \r\n\t\t\t\t\t\t\tage,\r\n\t\t\t\t\t\t\tgender,\r\n\t\t\t\t\t\t\trace,\r\n\t\t\t\t\t\t\tmarital_status,\r\n\t\t\t\t\t\t\tbmi,\r\n\t\t\t\t\t\t\tmed_cond1,\r\n\t\t\t\t\t\t\tmed_cond2,\r\n\t\t\t\t\t\t\tmed_cond3,\r\n\t\t\t\t\t\t\tmed_cond4,\r\n\t\t\t\t\t\t\tmed_cond5,\r\n\t\t\t\t\t\t\tmed_cond6,\r\n\t\t\t\t\t\t\tmed_cond7\t\t\r\n\t\t\t\t\t\t      ) as infection_cost_predicted \r\n\t\t\tfrom gpdemo.plr_mdls mdls, \r\n\t\t\t     gpdemo.patient_history_test test \r\n\t\t) q1\r\n\t) q2 group by cnt;\r\n```\r\n\r\n### <a name=\"parallelization\"/> Verify Parallelization\r\nCongratulations, you've just parallelized your first PL/R algorithm in GPDB/HAWQ!  Or have you? In this section we will describe three sanity checks to ensure that your code is actually running in parallel. \r\n\r\n\r\n#### <a name=\"plr_parallelization_hostnames\"/> Option 1: Via Segment Hostnames \r\nWe can quickly verify if a PL/R function is indeed running on all segment as follows:\r\n\r\n```SQL\r\ndrop function if exists plr_parallel_test;\r\ncreate function plr_parallel_test() \r\nreturns text \r\nas \r\n$$ \r\n\treturn (system('hostname',intern=TRUE)) \r\n$$ language 'plr';\r\n```\r\n\r\nThe function returns the hostname of the segment node on which it is executing. By invoking the function for rows from a table that is distributed across all segments, we can verify if we indeed\r\nsee all the segments in the output.\r\n\r\n```SQL\r\ngpadmin=# select distinct plr_parallel_test() from abalone;\r\n plr_parallel_test \r\n------------------\r\n sdw1\r\n sdw10\r\n sdw11\r\n sdw12\r\n sdw13\r\n sdw14\r\n sdw15\r\n sdw16\r\n sdw2\r\n sdw3\r\n sdw4\r\n sdw5\r\n sdw6\r\n sdw7\r\n sdw8\r\n sdw9\r\n(16 rows)\r\n```\r\n\r\nWe can see that all 16 segment hosts were returned in the result, which means all nodes executed our PL/R function.\r\n\r\n#### <a name=\"plr_parallelization_timing\"/> Option 2: Via Timing\r\nAn alternative way to verify whether your code is running in parallel is to do timed performance testing. This method is laborious, but can be helpful in precisely communicating the speedup achieved through parallelization to a business partner or customer. Using the abalone dataset, we show how to compare the timing results from an implementation that builds models sequentially with a version that builds models in parallel. \r\n\r\nFirst we create a PL/R function which builds a linear regression to predict the age of an abalone (determined by counting the number of rings) from physical measurements. The function returns the coefficients for each of the linear predictors. \r\n\r\n```SQL\r\n    DROP FUNCTION IF EXISTS plr_lm( sex text[], length float8[], diameter float8[],\r\n            height float8[], whole_weight float8[], \r\n            shucked_weight float8[], viscera_weight float8[], \r\n            shell_weight float8[], rings float8[] );\r\n    CREATE OR REPLACE FUNCTION plr_lm( sex text[], length float8[], \r\n            diameter float8[], height float8[], whole_weight float8[], \r\n            shucked_weight float8[], viscera_weight float8[], \r\n            shell_weight float8[], rings float8[] ) \r\n    RETURNS FLOAT8[] AS \r\n    $$\r\n      abalone   = data.frame( sex, length, diameter, height, whole_weight, \r\n            shucked_weight, viscera_weight, shell_weight, rings ) \r\n\r\n      m = lm(formula = rings ~ ., data = abalone)\r\n\r\n      coef( m )\r\n    $$\r\n    LANGUAGE 'plr';\r\n```\r\n\r\nNext we convert the dataset to an array representation (as described in [Data Preparation](#dataprep)) and store the results in a new table called `abalone_array`.\r\n\r\n```SQL\r\n    -- Create a vectorized version of the data\r\n    -- This table has a single row, and 9 columns\r\n    -- Each element contains all of the elements for the\r\n    -- respective column as an array \r\n    DROP TABLE IF EXISTS abalone_array;\r\n    CREATE TABLE abalone_array AS \r\n    SELECT \r\n      array_agg(sex)::text[] as sex, \r\n      array_agg(length)::float8[] as length,\r\n      array_agg(diameter)::float8[] as diameter, \r\n      array_agg(height)::float8[] as height,\r\n      array_agg(whole_weight)::float8[] as whole_weight, \r\n      array_agg(shucked_weight)::float8[] as shucked_weight,\r\n      array_agg(viscera_weight)::float8[] as viscera_weight, \r\n      array_agg(shell_weight)::float8[] as shell_weight, \r\n      array_agg(rings)::float8[] as rings\r\n    FROM abalone\r\n    DISTRIBUTED RANDOMLY;\r\n```\r\n\r\nNow that we have a PL/R function definition and the dataset prepared in an array representation, we can call the function like this:\r\n\r\n```\r\n    SELECT plr_lm( sex, length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight, rings )\r\n    FROM abalone_array;\r\n    ---------------\r\n    (1 row)\r\n\r\n    Time: 47.341 ms\r\n```\r\n\r\nNote that creating a single model takes about 47 ms. \r\n\r\nBut what if we want to create multiple models? For instance, imagine the abalone were sampled from 64 different regions and we hypothesize that the physical characteristics vary based on region. In this situation, we may want to construct multiple models to capture the region-specific effects. To simulate this scenario we will simply replicate the same dataset 64 times and build 64 identical models. We construct the models sequentially and in parallel and compare the execution time. \r\n\r\nTo build the models sequentially we create a simple PGSQL function that builds linear models in a loop using the `plr_lm` function we created earlier: \r\n\r\n```SQL\r\n    DROP FUNCTION IF EXISTS IterativePLRModels( INTEGER );\r\n    CREATE OR REPLACE FUNCTION IterativePLRModels( INTEGER ) \r\n    RETURNS SETOF TEXT \r\n    AS $BODY$\r\n    DECLARE\r\n      n ALIAS FOR $1;\r\n    BEGIN\r\n      FOR i IN 1..n LOOP\r\n        RAISE NOTICE 'Processing %', i;\r\n        PERFORM plr_lm( sex, length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight, rings )\r\n        FROM abalone_array;\r\n        RETURN NEXT i::TEXT;\r\n      END LOOP;\r\n    END\r\n    $BODY$\r\n      LANGUAGE plpgsql;\r\n```\r\n\r\nThe function accepts a single argument, which specifies the number of iterations. For this example we set that value to 64 and expect that the running time will be roughly the length of time it took to build a single model multiplied by the number of iterations: 47 * 64 = 3008 ms.\r\n\r\n```SQL\r\n    SELECT IterativePLRModels( 64 );\r\n    -----------\r\n    (64 rows)\r\n\r\n    Time: 2875.609 ms\r\n```\r\n\r\nPretty darn close!\r\n\r\nNext let's construct the models in parallel. In order to do this we must replicate the abalone data and distribute it across the GPDB segments. The PGSQL function below creates a new table called `abalone_array_replicates` that contains copies of the abalone dataset indexed by a `distkey` and distributed randomly across the segments. \r\n\r\n```SQL\r\n    DROP FUNCTION IF EXISTS ReplicateAbaloneArrays( INTEGER );\r\n    CREATE OR REPLACE FUNCTION ReplicateAbaloneArrays( INTEGER ) \r\n    RETURNS INTEGER AS\r\n    $BODY$\r\n    DECLARE\r\n      n ALIAS FOR $1;\r\n    BEGIN\r\n      DROP TABLE IF EXISTS abalone_array_replicates;\r\n      CREATE TABLE abalone_array_replicates AS\r\n      SELECT 1 as distkey, * FROM abalone_array\r\n      DISTRIBUTED randomly;\r\n\r\n      FOR i IN 2..n LOOP\r\n        INSERT INTO abalone_array_replicates SELECT i as distkey, * FROM abalone_array;\r\n      END LOOP;\r\n\r\n      RETURN n;\r\n    END;\r\n    $BODY$\r\n      LANGUAGE plpgsql;\r\n```\r\n\r\nThe function accepts a single argument, which specifies the number of copies to make: \r\n```\r\n    -- Create 64 copies\r\n    SELECT ReplicateAbaloneArrays( 64 );\r\n```\r\n\r\nNow we have a new table `abalone_array_replicates` that contains 64 rows and 9 columns in array representation, simulating measurements of 64 different types of abalone collected from different regions. We are now ready to construct 64 models in parallel. If the parallelization were perfectly efficient, the expected running time would be the running time of a single model, multiplied by the number of models, divided by the number of segments: (47 * 64) / 96 ~= 31 ms!\r\n\r\n```SQL\r\n    SELECT plr_lm( sex, length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight, rings )\r\n    FROM abalone_array_replicates;\r\n    -----------------\r\n    (64 rows)\r\n\r\n    Time: 183.937 ms\r\n```\r\n\r\nOf course, parallelization aint perfect! There is overhead associated with parallel processing. However, the contribution of the overhead to the overall running time of an algorithm shrinks as the size of the data increase. Additionally, since the distribution function is `random` data are not necessarily *uniformly* distributed across segments. You can see how the data are distributed by interrogating the database like this:\r\n\r\n```SQL\r\nSELECT gp_segment_id, count(*)\r\nFROM abalone_array_replicates\r\nGROUP BY gp_segment_id\r\nORDER BY gp_segment_id;\r\n```\r\n\r\nIf you plot the results in R:\r\n\r\n```splus\r\nbarplot( segment_distribution, xlab='Segment ID', ylab='Number of rows', main = 'Row distribution w/ sequential dist key' )\r\n```\r\n\r\nYou will get a plot that looks something like the one below. Note that certain segments (64, 61) have 3 models to build, while others only have 1. The overall running time of the algorithm is bounded by the running time of the slowest node - a good reminder of why it is important to choose your distribution key wisely!\r\n\r\n![alt text](https://github.com/zimmeee/gp-r/blob/master/figures/RowDistAcrossSegments.png?raw=true \"Row distribution across segments\")\r\n\r\n#### <a name=\"plr_parallelization_cc\"/> Option 3: Via Pivotal Command Center \r\nA heuristic, visual option to verify parallelism is via the Pivotal Command Center.  You would want to start by logging into Pivotal Command Center, and navigating to the 'Realtime (By Server)' menu under the 'System Metrics' tab.  Below is an example of how this page should look if your database is idle:\r\n\r\n![alt text](https://github.com/wjjung317/gp-r/blob/master/figures/commandcenter_idle.png?raw=true \"Snapshot of Pivotal Command Center When DB is Idle\")\r\n\r\nSuppose that you have now successfully implemented a parallelized PL/R function.  While the function is executing, check back on that same page on Pivotal Command Center - it should look like the following.  Note that the CPU panel shows activity for multiple database segments - if the function was not successfully parallelized, then only a single segment would show CPU activity.\r\n\r\n![alt text](https://github.com/wjjung317/gp-r/blob/master/figures/commandcenter_parallelized.png?raw=true \"Snapshot of Pivotal Command Center When DB is Executing a Parallelized PL/R Function\")\r\n\r\n\r\n\r\n## <a name=\"plr_details\"/> More Details\r\n\r\n### <a name=\"datatypes\"/> Data Types\r\nAt its core, a function takes in input, does something with this input, and produces output.  PL/R functions in GPDB/HAWQ:\r\n\r\n1.\tTake SQL data types as input\r\n2.\tConverts SQL data types to R data types\r\n3.\tOutputs results as R data types\r\n4.\tConverts the R data type output as SQL data types\r\n\r\n(1) and (3) are fairly straightforward.  We personally found (2) and (4) a little less straightforward, and would like to devote some space to go into these two pieces in more detail.  \r\n\r\nThe purpose of this section is really to just help users be aware of default data type conversions, and keep them in mind when doing code development and debugging.\r\n\r\nIt is our subjective view that being familiar with the treatment of multi-element data types is generally more useful for day-to-day data science.  We focus on PL/R’s default treatment of multi-element numeric data types rather than scalars or text values.  Material on scalars and text will soon follow.  \r\n\r\n#### <a name=\"plr_datatypes_input\"/> PL/R Input Conversion: SQL Data Types → R Data Types\r\n\r\nWe will describe how SQL data types are converted into R data types via PL/R in this section.  \r\n\r\nLet’s take a look at some examples.  We first define a PL/R function that simply returns a string of identifying the R data type:\r\n```SQL\r\nDROP FUNCTION IF EXISTS func_array(arg_array float8[]);\r\nCREATE FUNCTION func_array(arg_array float8[]) \r\nRETURNS text AS \r\n$$ \r\nd<- arg_array\r\nreturn(class(d))\r\n$$\r\nLANGUAGE 'plr';\r\n```\r\n\r\nYou would think that 1D SQL arrays (i.e. a vector of values) should map to R vectors, but we see that 1D SQL arrays default-map to 1D R arrays:\r\n```SQL\r\nSELECT array[1,2,3,4];\r\n   array   \r\n-----------\r\n {1,2,3,4}\r\n(1 row)\r\n\r\nSELECT func_array(array[1,2,3,4]);\r\nfunc_array \r\n------------\r\n array\r\n(1 row)\r\n\r\n```\r\nGiven the result for 1D SQL arrays, what are your bets on how 2D SQL arrays are mapped to R objects?  Turns out that 2D SQL arrays (i.e. a matrix) default-map to R matrices (not R 2D arrays):\r\n```SQL\r\nSELECT array[array[1,2], array[3,4]];\r\n     array     \r\n---------------\r\n {{1,2},{3,4}}\r\n(1 row)\r\nSELECT func_array(array[array[1,2],array[3,4]]);\r\n func_array \r\n------------\r\n matrix\r\n(1 row)\r\n```\r\n\r\nAnd as one would expect, 3D SQL arrays map to an R array:\r\n```SQL\r\nSELECT array[array[array[1,2], array[3,4]],array[array[5,6], array[7,8]]];\r\n             array             \r\n-------------------------------\r\n {{{1,2},{3,4}},{{5,6},{7,8}}}\r\n(1 row)\r\nSELECT func_array(array[array[array[1,2], array[3,4]],array[array[5,6], array[7,8]]]);\r\nfunc_array \r\n------------\r\n array\r\n(1 row)\r\n```\r\n\r\nYou can of course convert between data types in R, so if an R function that you’d like to use in your workflow expects data to be in a certain R class, just make appropriate conversions in your PL/R code:\r\n```SQL\r\nDROP FUNCTION IF EXISTS func_convert_example(arg_array float8[]);\r\nCREATE FUNCTION func_convert_example(arg_array float8[]) \r\nRETURNS text AS \r\n$$ \r\nd<- arg_array\r\nd<- as.data.frame(d)\r\nreturn(class(d))\r\n$$\r\nLANGUAGE 'plr';\r\n\r\nSELECT func_convert_example(array[array[1,2], array[3,4]]); \r\nfunc_convert_example \r\n----------------------\r\n data.frame\r\n(1 row)\r\n```\r\n\r\n#### <a name=\"plr_datatypes_output\"/> PL/R Output Conversion: R Data Types → SQL Data Types\r\nFor multi-element returns from a PL/R function, you generally have two options.  Multi-element return objects from PL/R can be expressed as:\r\n\r\n1.\tA SQL array (in all flavors: 1D,2D,3D), or \r\n2.\tA SQL composite type\r\n\r\nThe quickest, “hands-free” approach is to just specify your return object as a SQL array.  Regardless of whether your R object is a vector, matrix, data.frame, or array, you will be able to recover the information contained in the R object by specifying a SQL array as your RETURN data type for a given PL/R function.\r\n\r\n* Vectors, a single column of a matrix or data.frame, and a 1D R array are returned as a 1D SQL array\r\n* A matrix, a data.frame, and a 2D R array are returned as a 2D SQL array\r\n* A 3D R array is returned as a 3D SQL array\r\n\r\nA couple of caveats here.  Arrays can be somewhat difficult to look at in SQL.  Also, there currently isn’t support for arrays of mixed type.  You can nominally set your return type to a text[], but this will find limited use in an analytics workflow.\r\n\r\nA richer, more flexible approach is to use a SQL composite type as your RETURN data type for a given PL/R function.  Let’s suppose you wanted to return the equivalent of an R data frame in your PL/R function.  In other words, lets suppose you’d like to return a table where at least one of the columns contains text rather than numbers.  We allow for this return by first setting up a SQL composite type in Greenplum.  You can think of SQL composite types as a “template” or “skeleton” for SQL tables.  When setting up a type, it’s useful to think ahead and draw out the format of the output you intend to get back from your PL/R function.\r\n```SQL\r\nDROP TYPE IF EXISTS iris_type CASCADE;\r\nCREATE TYPE iris_type AS (\r\nsepal_length float8, sepal_width float8, petal_length float8, petal_width float8, specices text);\r\n```\r\nWe can then return output from a PL/R function which follows the structure of the type you’ve created.  You just need to specify your return type as a SETOF your custom type:\r\n```SQL\r\nCREATE OR REPLACE FUNCTION iris_trivial ()  \r\nRETURNS SETOF iris_type AS \r\n$$ \r\ndata(iris)\r\nd<- iris\r\nreturn(d[c(1,51,100),])\r\n$$\r\nLANGUAGE 'plr';\r\n\r\nSELECT * from iris_trivial();\r\nsepal_length | sepal_width | petal_length | petal_width |  specices  \r\n--------------+-------------+--------------+-------------+------------\r\n          5.1 |         3.5 |          1.4 |         0.2 | setosa\r\n            7 |         3.2 |          4.7 |         1.4 | versicolor\r\n          5.7 |         2.8 |          4.1 |         1.3 | versicolor\r\n(3 rows)\r\n```\r\n\r\nThe data types for the individual columns are governed by those of the SQL composite defined:\r\n\r\n```SQL\r\nDROP TABLE IF EXISTS iris_trivial_table;\r\nCREATE TABLE iris_trivial_table AS SELECT * FROM iris_trivial();\r\n\\d+ iris_trivial_table\r\n                  Table \"public.iris_trivial_table\"\r\n    Column    |       Type       | Modifiers | Storage  | Description \r\n--------------+------------------+-----------+----------+-------------\r\n sepal_length | double precision |           | plain    | \r\n sepal_width  | double precision |           | plain    | \r\n petal_length | double precision |           | plain    | \r\n petal_width  | double precision |           | plain    | \r\n specices     | text             |           | extended | \r\n```\r\n\r\nWe see that this is identical to the set of column data types of iris_type.\r\n\r\n### <a name=\"memory\"/> Memory Limits\r\nWhen coding in PL/R there are a couple of memory management items to keep in mind.  \r\n\r\nRecall that R is installed on each and every host of the Greenplum database - one corrollary is that each \"mapper\" job which you wish to execute in parallel via PL/R must fit in the memory of the R on each host.  \r\n\r\nGiven the heavy use of arrays in a PL/R workflow, another item to keep in mind is that the maximum memory limit for each cell (i.e. each record-column tuple) in GPDB/HAWQ database is 1GB.  This is a theoretical upper bound and in practice, the maximum can be less than 1GB.  \r\n\r\n\r\n# <a name=\"plrexercises\"/>PL/R Exercises\r\nThe folder [PL/R Exercises](https://github.com/pivotalsoftware/gp-r/tree/master/exercises) contains 3 different exercises we developed for the [Data Science in Practice](http://pivotal.io/training) course using publicly available datasets. The first demonstrates\r\nusing Ridge Regression from the `MASS` package, the second demonstrates using decision trees in the `rpart` package while the third is an exercises onRandom Forests using the `randomForest` package. The solutions are included inline in the same file. \r\n\r\n# <a name=\"rpostgresql\"/> RPostgreSQL on Greenplum & HAWQ\r\n## Overview\r\nThe [RPostgreSQL package](http://cran.r-project.org/web/packages/RPostgreSQL/index.html) provides a database interface and PostgreSQL driver for R that is compatible with GDPB/HAWQ. This connection can be used to query GDPB/HAWQ in the normal fashion from within R code. We have found this package to be helpful for prototyping, working with datasets that can fit in-memory, and building visualizations. Generally speaking, using the RPostgreSQL interface does not lend itself to parallelization.  \r\n\r\nUsing RPostgreSQL with a database includes the following 3 steps: \r\n\r\n1.      Create a database driver for PostgreSQL, \r\n2.      Connect to a specific database, and \r\n3.      Execute the query on GPDB/HAWQ and return results \r\n\r\n## <a name=\"rpostgresql_local\"/> Local Development\r\nRPostgreSQL can be used in a local development environment to connect to a remote GPDB/HAWQ instance. Queries are processed in parallel on GPDB/HAWQ and results are returned in the familiar R data frame format. Use caution when returning large resultsets as you may run into the memory limitations of your local R instance. To ease troubleshooting, it can be helpful to develop/debug the SQL using your GPDB tool of choice (e.g. pgAdmin) before using it in R. \r\n\r\n```splus\r\n    DBNAME = 'marketing'\r\n    HOST   = '10.110.134.123'\r\n\r\n    # Create a driver\r\n    drv <- dbDriver( \"PostgreSQL\" )\r\n    # Create the database connection\r\n    con <- dbConnect( drv, dbname = DBNAME, host = HOST )\r\n\r\n    # Create the SQL query string. Include a semi-colon to terminate\r\n    querystring =   'SELECT countryname, income, babies FROM country_table;'\r\n    # Execute the query and return results as a data frame\r\n    countries   = dbGetQuery( con, querystring )\r\n\r\n    # Plot the results\r\n    plot( countries$income, countries$babies )\r\n```\r\n\r\n## <a name=\"plotting\"/> Plotting\r\nIt is probably best to do plotting on a single node (either the master or locally using the RPostgreSQL interface). In this context, plotting is no different from normal plotting in R. Of course, you likely have *a lot* of data which may obscure traditional visualization techniques. You may choose to experiment with packages like [bigviz](https://github.com/hadley/bigvis) which provides tools for exploratory data analysis of large datasets. \r\n\r\n## <a name=\"rpostgresql_plrcaveats\"/> Caveats Around Usage Within PL/R \r\nRPostgreSQL can also be used from within a PL/R function and deployed on the host GPDB instance. This bypasses the PL/R pipe for data exchange in favor of the DBI driver used by RPostgreSQL. The primary benefit of using this interface over the standard PL/R interface is that datatype conversions happen automatically; one need not specify all of the columns and their datatypes to pass to the function ahead of time. Sensible conversions are done automatically, including conversion of strings to factors which can be helpful in downstream processes. \r\n\r\nWhile RPostgreSQL can be quite useful in a development context, don't be fooled. It is not a good path towards actual parallelization of your R code. Because the code in the PL/R function accesses database objects it cannot safely be called in a distributed manner. This will lead to errors such as:\r\n\r\n```SQL\r\n    DROP FUNCTION IF EXISTS my_plr_error_func( character );\r\n    CREATE OR REPLACE FUNCTION my_plr_error_func( character ) \r\n    RETURNS INTEGER AS \r\n    $$\r\n      library(\"RPostgreSQL\")\r\n\r\n      drv <- dbDriver( \"PostgreSQL\" )\r\n      con <- dbConnect( drv, dbname = arg1 )\r\n\r\n      querystring = 'SELECT reviewid FROM sample_model_data;'\r\n      model.data  = dbGetQuery( con, querystring )\r\n\r\n      16\r\n    $$\r\n    LANGUAGE 'plr';\r\n```\r\n\r\nThis returns without error, but does not run in parallel\r\n```SQL\r\n    SELECT my_plr_error_func( 'zimmen' );\r\n```\r\n\r\nThis produces the error below\r\n```\r\n    SELECT my_plr_error_func( 'zimmen' ) \r\n    FROM sample_model_data;\r\n\r\n    ********** Error **********\r\n\r\n    ERROR: R interpreter expression evaluation error  (seg55 slice1 sdw3:40001 pid=1676)\r\n    SQL state: 22000\r\n    Detail: \r\n         Error in pg.spi.exec(sql) : \r\n      error in SQL statement : function cannot execute on segment because it accesses relation \"public.sample_model_data\"\r\n         In R support function pg.spi.exec\r\n    In PL/R function my_plr_error_func\r\n```\r\n\r\nGPDB is complaining because you are trying to access a table directly from a segment, which breaks the whole notion of coordination between the master node and its segments. Therefore, you cannot specify a `FROM` clause in your PL/R function when you make an RPostgreSQL call from within that function. \r\n\r\n#### Alternative\r\nFor the adventerous, the RPostgreSQL package provides more granular control over execution. An equivalent to dbGetQuery is to first submit the SQL to the database engine using dbSendQuery and then fetch the results: \r\n\r\n```splus\r\ndrv <- dbDriver( \"PostgreSQL\" )\r\ncon <- dbConnect( drv )\r\nres <- dbSendQuery( con, \"SELECT * FROM sample_model_data;\" )\r\ndata <- fetch( res, n = -1 ) \r\n```\r\n\r\nNote that the fetch function has a parameter, `n`, which sets the maximum number of records to retrieve. You probably always want to set this value to -1 to retrieve all of the records. I'm not sure why you would ever use this instead of the simpler dbGetQuery. \r\n\r\n# <a name=\"pivotalr\"/> PivotalR on Greenplum & HAWQ\r\n## Introduction\r\n[Apache MADlib](http://madlib.incubator.apache.org) is an open-source library for highly scalable in-database/in-Hadoop analytics, and it currently runs on GPDB, HAWQ, and PostgreSQL.  MADlib provides implicitly parallelized SQL implementations of statistical & machine learning models that run directly inside of GPDB, HAWQ, and PostgreSQL. Examples of algorithms currently available in MADlib include linear regression, logistic regression, multinomial regression, elastic net, ARIMA, k-means clustering, naïve bayes, decision trees, random forests, support vector machines, Cox proportional hazards, time series analysis, conditional random fields, association rules, and latent dirichlet allocation.  \r\n\r\nWhile end users benefit from MADlib’s high performance and scalability, its audience has previously been focused to those who are comfortable with modeling in SQL. [PivotalR](http://cran.r-project.org/web/packages/PivotalR/) is an R package that allows practitioners who know R but very little SQL to leverage the performance and scalability benefits of in-database/in-Hadoop processing.  \r\n\r\nThe debut release of PivotalR was shipped out in June 2013.  A quickstart guide to PivotalR is available [here](https://github.com/wjjung317/gp-r/blob/master/docs/PivotalR-quick-start%20v2.pdf).  There is active ongoing development of  PivotalR, and we encourage you to view or contribute to this work on its [GitHub Page](https://github.com/pivotalsoftware/PivotalR).\r\n\r\n## <a name=\"pivotalr_design\"/> Design & Features\r\n![alt text](https://github.com/wjjung317/gp-r/blob/master/figures/PivotalR.png?raw=true \"PivotalR Design\")\r\n\r\nAt its core, an R function in PivotalR:\r\n\r\n1. Translates R model formulas into corresponding SQL statements\r\n2. Executes these statements on the cluster\r\n3. Returns summarized model output to R \r\n\r\nThis allows R users to leverage the scalability and performance of in-database/in-Hadoop analytics without leaving the R command line. All of the computational heavy lifting is executed in-database, while the end user benefits from a familiar R interface.  Compared with respective native R functions, we observe a dramatic increase in scalability and a decrease in running time, even after normalizing for hardware differences. Furthermore, data movement -- which can take hours/days for big data -- is eliminated via PivotalR.  \r\n\r\nKey features include the following:\r\n\r\n* All data stays in DB: R objects merely point to DB objects\r\n* All model estimation and heavy lifting done in DB via MADlib \r\n* R → SQL translation done via PivotalR\r\n* Only strings of SQL and model output transferred across RPostgreSQL -- trivial data transfer\r\n\r\n## <a name=\"pivotalr_demo\"/> Demo\r\n\r\nWe have put together a [video demo](http://www.youtube.com/watch?v=6cmyRCMY6j0) of the debut release of PivotalR.  We also provide the [deck](https://github.com/wjjung317/gp-r/blob/master/docs/PivotalR_Demo.pptx), [code](https://github.com/wjjung317/gp-r/blob/master/src/R/PivotalR_Demo.R), and [data](https://drive.google.com/file/d/0B76GEdSVCa8NUlZhQnFBaGgyTk0/view?usp=sharing) used in the demo. Note that the demo intends to highlight a selection of functionality in PivotalR - we encourage you to check out the [documentation](http://cran.r-project.org/web/packages/PivotalR/PivotalR.pdf) and this [paper](https://journal.r-project.org/archive/2014-1/qian.pdf) published in the R Journal to explore more of its features.  \r\n\r\n## <a name=\"pivotalr_install\"/> Download & Installation\r\n\r\nPivotalR is available for download and installation from [CRAN](http://cran.r-project.org/web/packages/PivotalR/) and its [GitHub Page](https://github.com/gopivotal/PivotalR).\r\n\r\n\r\n# <a name=\"shiny_cf\"/> Shiny Apps on Cloud Foundry [Work in Progress]\r\n\r\n## Overview\r\n\r\nIn this guide, we will assume that the reader is familiar with the [Shiny](http://shiny.rstudio.com/) framework for building apps and dashboards.  Background on Shiny and those who need a refresher are encouraged to look [here](http://shiny.rstudio.com/tutorial/).  \r\n\r\nWe place our focus on helping you get started on hosting Shiny apps on Cloud Foundry.  Please keep in mind that the authors of this current page are data scientists, not application developers.  The instructions here are intended merely to help get you started -- readers are encouraged to consult other resources (i.e. [here](http://12factor.net/)) and ideally your [developer & designer](http://pivotallabs.com) buddies to improve and optimize.\r\n\r\n## Bare Mininum Requirements for Hosting Shiny Apps on CF\r\n* CF environment\r\n* R buildpack\r\n* Shiny Code\r\n* init.R file\r\n* manifest.yml file \r\n\r\n### CF environment\r\n* Set API endpoint \r\n* Login with your login/password\r\n\r\n### R buildpack\r\n* https://github.com/wjjung317/heroku-buildpack-r\r\n[more details to be filled out]\r\n\r\n### Shiny Code\r\n[more details to be filled out]\r\n\r\n### init.R file\r\n[more details to be filled out]\r\n\r\n### manifest.yml file\r\n[more details to be filled out]\r\n\r\n## Steps to push your shiny app to CF [more details to be filled out]\r\n1.  Make sure your shiny app works on your laptop\r\n2.  Call out required R libraries and other initialization settings in your init.R file\r\n3.  Reference your shiny app's runApp() R script in the manifest.yml file\r\n\r\n## Common mistakes to avoid\r\n* Don't assume that the latest version of buildpacks are good to go and when you push a new app or update an existing one -- you may need to mess around with the buildpack compile script.  After some trial-and-error, I needed to revert to an older version of the R buildpack as the latest version had a compatibility bug with one of the dependent libraries of Shiny.\r\n* When referring to file names in scripts that are used in your app, keep in mind case sensitivity of file names and file extensions.  \r\n\r\n# Authors and Contributors\r\n* Woo Jung (@wjjung317)\r\n* Srivatsan 'Vatsan' Ramanujam (@vatsan)\r\n* Noah Zimmerman (@zimmeee)\r\n* Alex Kagoshima (@alexkago)\r\n* Ronert Obst (@ronert)\r\n* Regunathan Radhakrishnan (@regu_r)\r\n",
  "google": "UA-39168204-1",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}